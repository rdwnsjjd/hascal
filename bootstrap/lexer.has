// The Hascal Lexer
//
// Copyright (c) 2019-2022 Hascal Foundation
// all rights reserved.
//
// wikipedia :
// In computer science, lexical analysis, lexing or tokenization is the process of 
// converting a sequence of characters (such as in a computer program or web page) into 
// a sequence of _tokens (strings with an assigned and thus identified meaning). 
// A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner,
// although scanner is also a term for the first stage of a lexer. 
// A lexer is generally combined with a parser, which together analyze the syntax of programming languages, 
// web pages, and so forth.

use strings
local use keywords

// token struct
struct token {
    var type : string
    var value : string
    var line : int
}

var _lexme = "" // current lexeme
var current = 0 // current character
var _source = "" // source code
var line = 1 // current line

var _tokens : [token]

// check if current character is at the end of the source code
function isAtEnd() : bool {
    if current >= len(_source) {
        return true
    }
    return false
}

// get next character
function advance() : char{
    current = current + 1
    return _source[current]
}

// get next character without advancing
function peek() : char {
    if isAtEnd() {
        return '\0'
    }
    return _source[current + 1]
}

// lexer error
function lexer_error(filename:string, msg:string,line:int) {
    var fmt_msg = format("{}: Illegal character '{}':{}", filename, msg, to_string(line))
}

// scan next token
function scan_token(filename:string){
    var c = advance()
    var tok : token

    // skip whitespace
    if c == ' ' {
        scan_token(filename)
    } 
    // skip newline(lines saved as a token, we need it in parser phase)
    else if c == '\n' {
        tok = token("NEWLINE", "", line)
        append(_tokens,tok)
        line = line + 1
    } 

    else if c == '(' {
        tok = ["LPAREN","(",line]
        append(_tokens,tok)
    } else if c == ')' {
        tok = ["RPAREN",")",line]
        append(_tokens,tok)
    } 
    
    else if is_alpha(c) or c == '_' {
        _lexme = to_string(c)
        while not isAtEnd() and (is_alpha(c) or is_number(c) or c == '_') {
            _lexme = _lexme + to_string(c)
            c = advance()
        }

        if check_keyword(_lexme) {
            tok = token(_lexme,_lexme,line)
        } else {
            tok = token("NAME",_lexme,line)
        }
    } 

    else if is_number(c) {
        _lexme = to_string(c)
        while not isAtEnd() and is_number(c) {
            _lexme = _lexme + to_string(c)
            c = advance()
        }
        tok = token("NUMBER", _lexme, line)
        append(_tokens,tok)
    } 
    else if c == '"' {
        _lexme = to_string(c)
        while not isAtEnd() and c != '"' {
            _lexme = _lexme + to_string(c)
            c = advance()
        }
        c = advance()
        tok = token("STRING", _lexme, line)
        append(_tokens,tok)
    } 
    
    else if c == '+' {
        tok = token("PLUS", "+", line)
        append(_tokens,tok)
    } 
    else if c == '-' {
        tok = token("MINUS", "-", line)
        append(_tokens,tok)
    } 
    else if c == '*' {
        tok = token("TIMES", "*", line)
        append(_tokens,tok)
    } 
    else if c == '/' {
        c = advance()
        // skip single line comments
        if c == '/' {
            while not isAtEnd() and c != '\n' {
                c = advance()
            }
        }
        // skip multi line comments
        else if c == '*' {
            while not isAtEnd() and c != '*' and c != '/' {
                c = advance()
            }
            c = advance()
        }
        // else it's a division
        else {
            tok = token("DIV", "/", line)
            append(_tokens,tok)
        }
    }

    else if c == '=' {
        c = peek()

        // check if it's a assign or equal
        if c == '=' {
            tok = token("EQUAL", "==", line)
            append(_tokens,tok)
        } else {
            tok = token("ASSIGN","=",line)
            append(_tokens,tok)
        }
    }
    else if c == '<' {
        c = peek()

        // check if it's a less than or less than equal
        if c == '=' {
            tok = token("LESSEQ", "<=", line)
            append(_tokens,tok)
        } else {
            tok = token("LESS","<",line)
            append(_tokens,tok)
        }
    }
    else if c == '>' {
        c = peek()

        // check if it's a greater than or greater than equal
        if c == '=' {
            tok = token("GREATEREQ", ">=", line)
            append(_tokens,tok)
        } else {
            tok = token("GREATER",">",line)
            append(_tokens,tok)
        }
    }
    else if c == '!' {
        c = peek()

        // check if it's a greater than or greater than equal
        if c == '=' {
            tok = token("GREATEREQ", ">=", line)
            append(_tokens,tok)
        } else {
            lexer_error(filename,to_string(c),line)
            exit(1)
        }
    }

    else if c == '.' {
        tok = token("DOT", ".", line)
        append(_tokens,tok)
    } 
    else if c == ':' {
        tok = token("COLON", ":", line)
        append(_tokens,tok)
    }
    else if c == ',' {
        tok = token("COMMA", ",", line)
        append(_tokens,tok)
    }

    else if c == '{' {
        tok = token("LBC", "{", line)
        append(_tokens,tok)
    }
    else if c == '}' {
        tok = token("RBC", "}", line)
        append(_tokens,tok)
    }

    else if c == '[' {
        tok = token("LBRCK", "[", line)
        append(_tokens,tok)
    }
    else if c == ']' {
        tok = token("RBRCK", "]", line)
        append(_tokens,tok)
    }
    
}
function tokenizer(filename:string,src:string): [token] {
    _source = src
    while isAtEnd() == false {
        scan_token(filename)
    }
    return _tokens
}
